
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>nonlinlogreg</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-09"><meta name="DC.source" content="nonlinlogreg.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">nonlinlogreg.m</a></li><li><a href="#2">Generate some data</a></li><li><a href="#3">Plot the data</a></li><li><a href="#4">Augment the data with nonlinear terms</a></li><li><a href="#5">Use the Newton-Raphson MAP solution</a></li><li><a href="#6">Plot the parameter convergence</a></li><li><a href="#7">Plot the decision contours</a></li></ul></div><h2>nonlinlogreg.m<a name="1"></a></h2><p>From A First Course in Machine Learning, Chapter 5. Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk] Logistic regression with nonlinear functions</p><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;
</pre><h2>Generate some data<a name="2"></a></h2><pre class="codeinput">x = [randn(50,2) + repmat([5 1],50,1)];
x = [x; randn(50,2)];
x = [x; randn(100,2) + repmat([2 3],100,1)];
t = [repmat(0,100,1);repmat(1,100,1)];
</pre><h2>Plot the data<a name="3"></a></h2><pre class="codeinput">ma = {<span class="string">'ko'</span>,<span class="string">'ks'</span>};
fc = {[0 0 0],[1 1 1]};
tv = unique(t);
figure(1); hold <span class="string">off</span>
<span class="keyword">for</span> i = 1:length(tv)
    pos = find(t==tv(i));
    plot(x(pos,1),x(pos,2),ma{i},<span class="string">'markerfacecolor'</span>,fc{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="nonlinlogreg_01.png" alt=""> <h2>Augment the data with nonlinear terms<a name="4"></a></h2><p><img src="nonlinlogreg_eq47580.png" alt="$w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2$"></p><pre class="codeinput">X = [x(:,1).^0 x x.^2];
</pre><h2>Use the Newton-Raphson MAP solution<a name="5"></a></h2><pre class="codeinput"><span class="comment">%Initisliase the parameters</span>
w = repmat(0,size(X,2),1); <span class="comment">% Start at zero</span>
tol = 1e-6; <span class="comment">% Stopping tolerance</span>
Nits = 100;
w_all = zeros(Nits,size(X,2)); <span class="comment">% Store evolution of w values</span>
ss = 10; <span class="comment">% Prior variance on the parameters of w</span>
change = inf;
it = 0;
<span class="keyword">while</span> change&gt;tol &amp; it&lt;=100
    prob_t = 1./(1+exp(-X*w));
    <span class="comment">% Gradient</span>
    grad = -(1/ss)*w' + sum(X.*(repmat(t,1,length(w))-repmat(prob_t,1,length(w))),1);
    <span class="comment">% Hessian</span>
    H = -X'*diag(prob_t.*(1-prob_t))*X;
    H = H - (1/ss)*eye(length(w));
    <span class="comment">% Update w</span>
    w = w - inv(H)*grad';
    it = it + 1;
    w_all(it,:) = w';
    <span class="keyword">if</span> it&gt;1
        change = sum((w_all(it,:) - w_all(it-1,:)).^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>
w_all(it+1:end,:) = [];
</pre><h2>Plot the parameter convergence<a name="6"></a></h2><pre class="codeinput">figure(1); hold <span class="string">off</span>
plot(w_all);
xlabel(<span class="string">'Iterations'</span>);
ylabel(<span class="string">'Parameters'</span>);
</pre><img vspace="5" hspace="5" src="nonlinlogreg_02.png" alt=""> <h2>Plot the decision contours<a name="7"></a></h2><pre class="codeinput">[Xv,Yv] = meshgrid(min(x(:,1)):0.1:max(x(:,1)),min(x(:,2)):0.1:max(x(:,2)));
Pvals = 1./(1 + exp(-(w(1) + w(2).*Xv + w(3).*Yv + w(4).*Xv.^2 + w(5).*Yv.^2)));
figure(1); hold <span class="string">off</span>
<span class="keyword">for</span> i = 1:length(tv)
    pos = find(t==tv(i));
    plot(x(pos,1),x(pos,2),ma{i},<span class="string">'markerfacecolor'</span>,fc{i});
    hold <span class="string">on</span>
<span class="keyword">end</span>
[c,h] = contour(Xv,Yv,Pvals,[0.1 0.5 0.9],<span class="string">'k'</span>);
clabel(c,h)
</pre><img vspace="5" hspace="5" src="nonlinlogreg_03.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% nonlinlogreg.m
% From A First Course in Machine Learning, Chapter 5.
% Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk]
% Logistic regression with nonlinear functions
clear all;close all;

%% Generate some data
x = [randn(50,2) + repmat([5 1],50,1)];
x = [x; randn(50,2)];
x = [x; randn(100,2) + repmat([2 3],100,1)];
t = [repmat(0,100,1);repmat(1,100,1)];

%% Plot the data
ma = {'ko','ks'};
fc = {[0 0 0],[1 1 1]};
tv = unique(t);
figure(1); hold off
for i = 1:length(tv)
    pos = find(t==tv(i));
    plot(x(pos,1),x(pos,2),ma{i},'markerfacecolor',fc{i});
    hold on
end

%% Augment the data with nonlinear terms
% $w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 + w_4x_2^2$
X = [x(:,1).^0 x x.^2];

%% Use the Newton-Raphson MAP solution
%Initisliase the parameters
w = repmat(0,size(X,2),1); % Start at zero
tol = 1e-6; % Stopping tolerance
Nits = 100;
w_all = zeros(Nits,size(X,2)); % Store evolution of w values
ss = 10; % Prior variance on the parameters of w
change = inf;
it = 0;
while change>tol & it<=100
    prob_t = 1./(1+exp(-X*w));
    % Gradient
    grad = -(1/ss)*w' + sum(X.*(repmat(t,1,length(w))-repmat(prob_t,1,length(w))),1);
    % Hessian
    H = -X'*diag(prob_t.*(1-prob_t))*X;
    H = H - (1/ss)*eye(length(w));
    % Update w
    w = w - inv(H)*grad';
    it = it + 1;
    w_all(it,:) = w';
    if it>1
        change = sum((w_all(it,:) - w_all(it-1,:)).^2);
    end
end
w_all(it+1:end,:) = [];


%% Plot the parameter convergence
figure(1); hold off
plot(w_all);
xlabel('Iterations');
ylabel('Parameters');

%% Plot the decision contours
[Xv,Yv] = meshgrid(min(x(:,1)):0.1:max(x(:,1)),min(x(:,2)):0.1:max(x(:,2)));
Pvals = 1./(1 + exp(-(w(1) + w(2).*Xv + w(3).*Yv + w(4).*Xv.^2 + w(5).*Yv.^2)));
figure(1); hold off
for i = 1:length(tv)
    pos = find(t==tv(i));
    plot(x(pos,1),x(pos,2),ma{i},'markerfacecolor',fc{i});
    hold on
end
[c,h] = contour(Xv,Yv,Pvals,[0.1 0.5 0.9],'k');
clabel(c,h)
##### SOURCE END #####
--></body></html>