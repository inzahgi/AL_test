
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>logmap</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-02"><meta name="DC.source" content="logmap.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">logmap.m</a></li><li><a href="#2">Load the classification data</a></li><li><a href="#3">Initisliase the parameters</a></li><li><a href="#4">Plot the evolution of w</a></li><li><a href="#5">Plot the probability contours</a></li></ul></div><h2>logmap.m<a name="1"></a></h2><p>From A First Course in Machine Learning, Chapter 4. Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk] Finding the MAP parameter value using logistic regression</p><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;
</pre><h2>Load the classification data<a name="2"></a></h2><pre class="codeinput">load <span class="string">../data/logregdata</span>

<span class="comment">% Plot the data</span>
figure(1);hold <span class="string">off</span>
plot(X(1:20,1),X(1:20,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'markerfacecolor'</span>,<span class="string">'k'</span>)
hold <span class="string">on</span>
plot(X(21:40,1),X(21:40,2),<span class="string">'ks'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
</pre><img vspace="5" hspace="5" src="logmap_01.png" alt=""> <h2>Initisliase the parameters<a name="3"></a></h2><pre class="codeinput">w = repmat(0,2,1); <span class="comment">% Start at zero</span>
tol = 1e-6; <span class="comment">% Stopping tolerance</span>
Nits = 100;
w_all = zeros(Nits,2); <span class="comment">% Store evolution of w values</span>
ss = 10; <span class="comment">% Prior variance on the parameters of w</span>
change = inf;
it = 0;
<span class="keyword">while</span> change&gt;tol &amp; it&lt;=100
    prob_t = 1./(1+exp(-X*w));
    <span class="comment">% Gradient</span>
    grad = -(1/ss)*w' + sum(X.*(repmat(t,1,length(w))-repmat(prob_t,1,length(w))),1);
    <span class="comment">% Hessian</span>
    H = -X'*diag(prob_t.*(1-prob_t))*X;
    H = H - (1/ss)*eye(length(w));
    <span class="comment">% Update w</span>
    w = w - inv(H)*grad';
    it = it + 1;
    w_all(it,:) = w';
    <span class="keyword">if</span> it&gt;1
        change = sum((w_all(it,:) - w_all(it-1,:)).^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>
w_all(it+1:end,:) = [];
</pre><h2>Plot the evolution of w<a name="4"></a></h2><pre class="codeinput">figure(1);hold <span class="string">off</span>
plot(w_all);
xlabel(<span class="string">'Iterations'</span>);
ylabel(<span class="string">'w'</span>);
</pre><img vspace="5" hspace="5" src="logmap_02.png" alt=""> <h2>Plot the probability contours<a name="5"></a></h2><pre class="codeinput">figure(1);hold <span class="string">off</span>
plot(X(1:20,1),X(1:20,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'markerfacecolor'</span>,<span class="string">'k'</span>)
hold <span class="string">on</span>
plot(X(21:40,1),X(21:40,2),<span class="string">'ks'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
[Xv,Yv] = meshgrid(-5:0.1:5,-5:0.1:5);
Probs = 1./(1+exp(-(w(1).*Xv + w(2).*Yv)));
[cs,h] = contour(Xv,Yv,Probs);
clabel(cs,h);
fprintf(<span class="string">'\nProbabilities are probabilities of belonging to the square class'</span>);
</pre><pre class="codeoutput">
Probabilities are probabilities of belonging to the square class</pre><img vspace="5" hspace="5" src="logmap_03.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% logmap.m
% From A First Course in Machine Learning, Chapter 4.
% Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk]
% Finding the MAP parameter value using logistic regression
clear all;close all;

%% Load the classification data
load ../data/logregdata

% Plot the data
figure(1);hold off
plot(X(1:20,1),X(1:20,2),'ko','markersize',10,'markerfacecolor','k')
hold on
plot(X(21:40,1),X(21:40,2),'ks','markersize',10,'linewidth',2)

%% Initisliase the parameters
w = repmat(0,2,1); % Start at zero
tol = 1e-6; % Stopping tolerance
Nits = 100;
w_all = zeros(Nits,2); % Store evolution of w values
ss = 10; % Prior variance on the parameters of w
change = inf;
it = 0;
while change>tol & it<=100
    prob_t = 1./(1+exp(-X*w));
    % Gradient
    grad = -(1/ss)*w' + sum(X.*(repmat(t,1,length(w))-repmat(prob_t,1,length(w))),1);
    % Hessian
    H = -X'*diag(prob_t.*(1-prob_t))*X;
    H = H - (1/ss)*eye(length(w));
    % Update w
    w = w - inv(H)*grad';
    it = it + 1;
    w_all(it,:) = w';
    if it>1
        change = sum((w_all(it,:) - w_all(it-1,:)).^2);
    end
end
w_all(it+1:end,:) = [];
    
%% Plot the evolution of w
figure(1);hold off
plot(w_all);
xlabel('Iterations');
ylabel('w');

%% Plot the probability contours
figure(1);hold off
plot(X(1:20,1),X(1:20,2),'ko','markersize',10,'markerfacecolor','k')
hold on
plot(X(21:40,1),X(21:40,2),'ks','markersize',10,'linewidth',2)
[Xv,Yv] = meshgrid(-5:0.1:5,-5:0.1:5);
Probs = 1./(1+exp(-(w(1).*Xv + w(2).*Yv)));
[cs,h] = contour(Xv,Yv,Probs);
clabel(cs,h);
fprintf('\nProbabilities are probabilities of belonging to the square class');
##### SOURCE END #####
--></body></html>