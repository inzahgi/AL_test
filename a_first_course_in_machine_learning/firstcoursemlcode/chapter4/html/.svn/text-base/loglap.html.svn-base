
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>loglap</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-02"><meta name="DC.source" content="loglap.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">loglap.m</a></li><li><a href="#2">Load the classification data</a></li><li><a href="#3">Find the mode and the Hessian (see logmap.m)</a></li><li><a href="#4">Set the Laplace approximation</a></li><li><a href="#5">Plot the true posterior (note that we can only get this in unnormalised form)</a></li><li><a href="#6">Overlay the approximation</a></li><li><a href="#7">Plot the decision contours</a></li></ul></div><h2>loglap.m<a name="1"></a></h2><p>From A First Course in Machine Learning, Chapter 4. Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk] The Laplace approximation for logistic regression</p><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;
</pre><h2>Load the classification data<a name="2"></a></h2><pre class="codeinput">load <span class="string">../data/logregdata</span>
</pre><h2>Find the mode and the Hessian (see logmap.m)<a name="3"></a></h2><pre class="codeinput">w = repmat(0,2,1); <span class="comment">% Start at zero</span>
tol = 1e-6; <span class="comment">% Stopping tolerance</span>
Nits = 100;
w_all = zeros(Nits,2); <span class="comment">% Store evolution of w values</span>
ss = 10; <span class="comment">% Prior variance on the parameters of w</span>
change = inf;
it = 0;
<span class="keyword">while</span> change&gt;tol | it&lt;=100
    prob_t = 1./(1+exp(-X*w));
    <span class="comment">% Gradient</span>
    grad = -(1/ss)*w' + sum(X.*(repmat(t,1,length(w))-repmat(prob_t,1,length(w))),1);
    <span class="comment">% Hessian</span>
    H = -X'*diag(prob_t.*(1-prob_t))*X;
    H = H - (1/ss)*eye(length(w));
    <span class="comment">% Update w</span>
    w = w - inv(H)*grad';
    it = it + 1;
    w_all(it,:) = w';
    <span class="keyword">if</span> it&gt;1
        change = sum((w_all(it,:) - w_all(it-1,:)).^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>
w_all(it+1:end,:) = [];
</pre><h2>Set the Laplace approximation<a name="4"></a></h2><pre class="codeinput">muw = w;
siw = inv(-H);
</pre><h2>Plot the true posterior (note that we can only get this in unnormalised form)<a name="5"></a></h2><pre class="codeinput">[w1,w2] = meshgrid(-5:0.1:5,-5:0.1:5);
logprior = -0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w1.^2;
logprior = logprior + (-0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w2.^2);
prob_t = 1./(1+exp(-[w1(:) w2(:)]*X'));
loglike = sum(log(prob_t).*repmat(t',prod(size(w1)),1),2);
loglike = loglike + sum(log(1-prob_t).*repmat(1-t',prod(size(w1)),1),2);
logpost = logprior + reshape(loglike,size(w1));
contour(w1,w2,exp(logpost),<span class="string">'k'</span>,<span class="string">'color'</span>,[0.6 0.6 0.6])
xlabel(<span class="string">'$w1$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>);
ylabel(<span class="string">'$w2$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>);
</pre><img vspace="5" hspace="5" src="loglap_01.png" alt=""> <h2>Overlay the approximation<a name="6"></a></h2><pre class="codeinput">temp = [w1(:)-muw(1) w2(:)-muw(2)];
D = 2; <span class="comment">% Working in 2 dimensions</span>
logconst = -(D/2)*log(2*pi) - 0.5*log(det(siw));
log_truepost = logconst - diag(0.5*temp*inv(siw)*temp');
hold <span class="string">on</span>
contour(w1,w2,reshape(exp(log_truepost),size(w1)),<span class="string">'k'</span>);
legend(<span class="string">'True'</span>,<span class="string">'Laplace Approximation'</span>);
</pre><img vspace="5" hspace="5" src="loglap_02.png" alt=""> <h2>Plot the decision contours<a name="7"></a></h2><pre class="codeinput"><span class="comment">% Create an x grid</span>
[Xv,Yv] = meshgrid(-5:0.1:5,-5:0.1:5);

<span class="comment">% Generate samples from the approximate posterior</span>
path(path,<span class="string">'../utilities'</span>);
Nsamps = 1000;
w_samps = gausssamp(muw,siw,Nsamps);

<span class="comment">% Compute the probabilities over the grid by averaging over the samples</span>
Probs = zeros(size(Xv));
<span class="keyword">for</span> i = 1:Nsamps
    Probs = Probs + 1./(1 + exp(-(w_samps(i,1)*Xv + w_samps(i,2)*Yv)));
<span class="keyword">end</span>
Probs = Probs./Nsamps;
figure(1);hold <span class="string">off</span>
plot(X(1:20,1),X(1:20,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'markerfacecolor'</span>,<span class="string">'k'</span>)
hold <span class="string">on</span>
plot(X(21:40,1),X(21:40,2),<span class="string">'ks'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
[cs,h] = contour(Xv,Yv,Probs);
clabel(cs,h);
</pre><img vspace="5" hspace="5" src="loglap_03.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% loglap.m
% From A First Course in Machine Learning, Chapter 4.
% Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk]
% The Laplace approximation for logistic regression
clear all;close all;

%% Load the classification data
load ../data/logregdata

%% Find the mode and the Hessian (see logmap.m)
w = repmat(0,2,1); % Start at zero
tol = 1e-6; % Stopping tolerance
Nits = 100;
w_all = zeros(Nits,2); % Store evolution of w values
ss = 10; % Prior variance on the parameters of w
change = inf;
it = 0;
while change>tol | it<=100
    prob_t = 1./(1+exp(-X*w));
    % Gradient
    grad = -(1/ss)*w' + sum(X.*(repmat(t,1,length(w))-repmat(prob_t,1,length(w))),1);
    % Hessian
    H = -X'*diag(prob_t.*(1-prob_t))*X;
    H = H - (1/ss)*eye(length(w));
    % Update w
    w = w - inv(H)*grad';
    it = it + 1;
    w_all(it,:) = w';
    if it>1
        change = sum((w_all(it,:) - w_all(it-1,:)).^2);
    end
end
w_all(it+1:end,:) = [];

%% Set the Laplace approximation
muw = w;
siw = inv(-H);

%% Plot the true posterior (note that we can only get this in unnormalised form)
[w1,w2] = meshgrid(-5:0.1:5,-5:0.1:5);
logprior = -0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w1.^2;
logprior = logprior + (-0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w2.^2);
prob_t = 1./(1+exp(-[w1(:) w2(:)]*X'));
loglike = sum(log(prob_t).*repmat(t',prod(size(w1)),1),2);
loglike = loglike + sum(log(1-prob_t).*repmat(1-t',prod(size(w1)),1),2);
logpost = logprior + reshape(loglike,size(w1));
contour(w1,w2,exp(logpost),'k','color',[0.6 0.6 0.6])
xlabel('$w1$','interpreter','latex');
ylabel('$w2$','interpreter','latex');
%% Overlay the approximation

temp = [w1(:)-muw(1) w2(:)-muw(2)];
D = 2; % Working in 2 dimensions
logconst = -(D/2)*log(2*pi) - 0.5*log(det(siw));
log_truepost = logconst - diag(0.5*temp*inv(siw)*temp');
hold on
contour(w1,w2,reshape(exp(log_truepost),size(w1)),'k');
legend('True','Laplace Approximation');

%% Plot the decision contours

% Create an x grid
[Xv,Yv] = meshgrid(-5:0.1:5,-5:0.1:5);

% Generate samples from the approximate posterior
path(path,'../utilities');
Nsamps = 1000;
w_samps = gausssamp(muw,siw,Nsamps);

% Compute the probabilities over the grid by averaging over the samples
Probs = zeros(size(Xv));
for i = 1:Nsamps
    Probs = Probs + 1./(1 + exp(-(w_samps(i,1)*Xv + w_samps(i,2)*Yv)));
end
Probs = Probs./Nsamps;
figure(1);hold off
plot(X(1:20,1),X(1:20,2),'ko','markersize',10,'markerfacecolor','k')
hold on
plot(X(21:40,1),X(21:40,2),'ks','markersize',10,'linewidth',2)
[cs,h] = contour(Xv,Yv,Probs);
clabel(cs,h);
##### SOURCE END #####
--></body></html>