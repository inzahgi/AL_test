
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>logmh</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-11-02"><meta name="DC.source" content="logmh.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">logmh.m</a></li><li><a href="#2">Load the classification data</a></li><li><a href="#3">Initialise w</a></li><li><a href="#4">Generate N Samples</a></li><li><a href="#5">Plot the true contours and the samples</a></li><li><a href="#6">Plot the prediction contours</a></li></ul></div><h2>logmh.m<a name="1"></a></h2><p>From A First Course in Machine Learning, Chapter 4. Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk] Metropolis-Hastings for Logistic Regression</p><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;
</pre><h2>Load the classification data<a name="2"></a></h2><pre class="codeinput">load <span class="string">../data/logregdata</span>;
</pre><h2>Initialise w<a name="3"></a></h2><pre class="codeinput">w = randn(2,1);
</pre><h2>Generate N Samples<a name="4"></a></h2><pre class="codeinput">ss = 10;
N = 2000;
jumpvar = 1; <span class="comment">% Jumping variance for each parameter</span>
w_all = zeros(N,2);
Naccept = 0;
<span class="keyword">for</span> n = 1:N
    <span class="comment">% Propose a new sample</span>
    ws = w + randn(2,1).*sqrt(jumpvar);
    <span class="comment">% Compute ratio of new to old priors (constants cancel)</span>
    priorrat = -(1/(2*ss))*ws'*ws;
    priorrat = priorrat + (1/(2*ss))*w'*w; <span class="comment">% Subtract old prior</span>
    <span class="comment">% Compute ratio of new to old likelihoods</span>
    prob = 1./(1+exp(-X*w));
    newprob = 1./(1+exp(-X*ws));
    like = sum(t.*log(prob) + (1-t).*log(1-prob));
    newlike = sum(t.*log(newprob) + (1-t).*log(1-newprob));
    rat = newlike - like + priorrat;
    <span class="keyword">if</span> rand&lt;=exp(rat)
        <span class="comment">% Accept</span>
        Naccept = Naccept + 1;
        w = ws;
    <span class="keyword">end</span>
    w_all(n,:) = w;
<span class="keyword">end</span>
fprintf(<span class="string">'\nAcceptance ratio: %g'</span>,Naccept/N);
</pre><pre class="codeoutput">
Acceptance ratio: 0.6845</pre><h2>Plot the true contours and the samples<a name="5"></a></h2><pre class="codeinput">[w1,w2] = meshgrid(-5:0.1:5,-5:0.1:5);
logprior = -0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w1.^2;
logprior = logprior + (-0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w2.^2);
prob_t = 1./(1+exp(-[w1(:) w2(:)]*X'));
loglike = sum(log(prob_t).*repmat(t',prod(size(w1)),1),2);
loglike = loglike + sum(log(1-prob_t).*repmat(1-t',prod(size(w1)),1),2);
logpost = logprior + reshape(loglike,size(w1));
contour(w1,w2,exp(logpost),<span class="string">'k'</span>,<span class="string">'color'</span>,[0.6 0.6 0.6])
xlabel(<span class="string">'$w1$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>);
ylabel(<span class="string">'$w2$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>);
hold <span class="string">on</span>

plot(w_all(:,1),w_all(:,2),<span class="string">'k.'</span>,<span class="string">'markersize'</span>,10)
</pre><img vspace="5" hspace="5" src="logmh_01.png" alt=""> <h2>Plot the prediction contours<a name="6"></a></h2><p>Create an x grid</p><pre class="codeinput">[Xv,Yv] = meshgrid(-5:0.1:5,-5:0.1:5);

<span class="comment">% Compute the probabilities over the grid by averaging over the samples</span>
Probs = zeros(size(Xv));
<span class="keyword">for</span> i = 1:N
    Probs = Probs + 1./(1 + exp(-(w_all(i,1)*Xv + w_all(i,2)*Yv)));
<span class="keyword">end</span>
Probs = Probs./N;
figure(1);hold <span class="string">off</span>
plot(X(1:20,1),X(1:20,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'markerfacecolor'</span>,<span class="string">'k'</span>)
hold <span class="string">on</span>
plot(X(21:40,1),X(21:40,2),<span class="string">'ks'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
[cs,h] = contour(Xv,Yv,Probs);
clabel(cs,h);
</pre><img vspace="5" hspace="5" src="logmh_02.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% logmh.m
% From A First Course in Machine Learning, Chapter 4.
% Simon Rogers, 01/11/11 [simon.rogers@glasgow.ac.uk]
% Metropolis-Hastings for Logistic Regression
clear all;close all;

%% Load the classification data
load ../data/logregdata;


%% Initialise w
w = randn(2,1);

%% Generate N Samples
ss = 10;
N = 2000;
jumpvar = 1; % Jumping variance for each parameter
w_all = zeros(N,2);
Naccept = 0;
for n = 1:N
    % Propose a new sample
    ws = w + randn(2,1).*sqrt(jumpvar);
    % Compute ratio of new to old priors (constants cancel)
    priorrat = -(1/(2*ss))*ws'*ws;
    priorrat = priorrat + (1/(2*ss))*w'*w; % Subtract old prior
    % Compute ratio of new to old likelihoods
    prob = 1./(1+exp(-X*w));
    newprob = 1./(1+exp(-X*ws));
    like = sum(t.*log(prob) + (1-t).*log(1-prob));
    newlike = sum(t.*log(newprob) + (1-t).*log(1-newprob));
    rat = newlike - like + priorrat;
    if rand<=exp(rat)
        % Accept
        Naccept = Naccept + 1;
        w = ws;
    end
    w_all(n,:) = w; 
end
fprintf('\nAcceptance ratio: %g',Naccept/N);

%% Plot the true contours and the samples

[w1,w2] = meshgrid(-5:0.1:5,-5:0.1:5);
logprior = -0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w1.^2;
logprior = logprior + (-0.5*log(2*pi) - 0.5*log(ss) - (1/(2*ss))*w2.^2);
prob_t = 1./(1+exp(-[w1(:) w2(:)]*X'));
loglike = sum(log(prob_t).*repmat(t',prod(size(w1)),1),2);
loglike = loglike + sum(log(1-prob_t).*repmat(1-t',prod(size(w1)),1),2);
logpost = logprior + reshape(loglike,size(w1));
contour(w1,w2,exp(logpost),'k','color',[0.6 0.6 0.6])
xlabel('$w1$','interpreter','latex');
ylabel('$w2$','interpreter','latex');
hold on

plot(w_all(:,1),w_all(:,2),'k.','markersize',10)

%% Plot the prediction contours
% Create an x grid
[Xv,Yv] = meshgrid(-5:0.1:5,-5:0.1:5);

% Compute the probabilities over the grid by averaging over the samples
Probs = zeros(size(Xv));
for i = 1:N
    Probs = Probs + 1./(1 + exp(-(w_all(i,1)*Xv + w_all(i,2)*Yv)));
end
Probs = Probs./N;
figure(1);hold off
plot(X(1:20,1),X(1:20,2),'ko','markersize',10,'markerfacecolor','k')
hold on
plot(X(21:40,1),X(21:40,2),'ks','markersize',10,'linewidth',2)
[cs,h] = contour(Xv,Yv,Probs);
clabel(cs,h);
##### SOURCE END #####
--></body></html>