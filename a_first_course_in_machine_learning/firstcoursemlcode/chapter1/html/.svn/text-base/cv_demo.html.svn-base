
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>cv_demo</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2011-10-31"><meta name="DC.source" content="cv_demo.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">cv_demo.m</a></li><li><a href="#2">Generate some data</a></li><li><a href="#3">Run a cross-validation over model orders</a></li><li><a href="#4">Plot the results</a></li></ul></div><h2>cv_demo.m<a name="1"></a></h2><p>From A First Course in Machine Learning, Chapter 1. Simon Rogers, 31/10/11 [simon.rogers@glasgow.ac.uk] Demonstration of cross-validation for model selection</p><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;
</pre><h2>Generate some data<a name="2"></a></h2><p>Generate x between -5 and 5</p><pre class="codeinput">N = 100;
x = 10*rand(N,1) - 5;
t = 5*x.^3  - x.^2 + x + 150*randn(size(x));
testx = [-5:0.01:5]'; <span class="comment">% Large, independent test set</span>
testt = 5*testx.^3 - testx.^2 + testx + 150*randn(size(testx));
</pre><h2>Run a cross-validation over model orders<a name="3"></a></h2><pre class="codeinput">maxorder = 7;
X = [];
testX = [];
K = 10 <span class="comment">%K-fold CV</span>
sizes = repmat(floor(N/K),1,K);
sizes(end) = sizes(end) + N - sum(sizes);
csizes = [0 cumsum(sizes)];

<span class="comment">% Note that it is often sensible to permute the data objects before</span>
<span class="comment">% performing CV.  It is not necessary here as x was created randomly.  If</span>
<span class="comment">% it were necessary, the following code would work:</span>
<span class="comment">% order = randperm(N);</span>
<span class="comment">% x = x(order); Or: X = X(order,:) if it is multi-dimensional.</span>
<span class="comment">% t = t(order);</span>

<span class="keyword">for</span> k = 0:maxorder
    X = [X x.^k];
    testX = [testX testx.^k];
    <span class="keyword">for</span> fold = 1:K
        <span class="comment">% Partition the data</span>
        <span class="comment">% foldX contains the data for just one fold</span>
        <span class="comment">% trainX contains all other data</span>

        foldX = X(csizes(fold)+1:csizes(fold+1),:);
        trainX = X;
        trainX(csizes(fold)+1:csizes(fold+1),:) = [];
        foldt = t(csizes(fold)+1:csizes(fold+1));
        traint = t;
        traint(csizes(fold)+1:csizes(fold+1)) = [];

        w = inv(trainX'*trainX)*trainX'*traint;
        fold_pred = foldX*w;
        cv_loss(fold,k+1) = mean((fold_pred-foldt).^2);
        ind_pred = testX*w;
        ind_loss(fold,k+1) = mean((ind_pred - testt).^2);
        train_pred = trainX*w;
        train_loss(fold,k+1) = mean((train_pred - traint).^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeoutput">
K =

    10

</pre><h2>Plot the results<a name="4"></a></h2><pre class="codeinput">figure(1);
subplot(131)
plot(0:maxorder,mean(cv_loss,1),<span class="string">'linewidth'</span>,2)
xlabel(<span class="string">'Model Order'</span>);
ylabel(<span class="string">'Loss'</span>);
title(<span class="string">'CV Loss'</span>);
subplot(132)
plot(0:maxorder,mean(train_loss,1),<span class="string">'linewidth'</span>,2)
xlabel(<span class="string">'Model Order'</span>);
ylabel(<span class="string">'Loss'</span>);
title(<span class="string">'Train Loss'</span>);
subplot(133)
plot(0:maxorder,mean(ind_loss,1),<span class="string">'linewidth'</span>,2)
xlabel(<span class="string">'Model Order'</span>);
ylabel(<span class="string">'Loss'</span>);
title(<span class="string">'Independent Test Loss'</span>)
</pre><img vspace="5" hspace="5" src="cv_demo_01.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% cv_demo.m
% From A First Course in Machine Learning, Chapter 1.
% Simon Rogers, 31/10/11 [simon.rogers@glasgow.ac.uk]
% Demonstration of cross-validation for model selection
clear all;close all;
%% Generate some data
% Generate x between -5 and 5
N = 100;
x = 10*rand(N,1) - 5;
t = 5*x.^3  - x.^2 + x + 150*randn(size(x));
testx = [-5:0.01:5]'; % Large, independent test set
testt = 5*testx.^3 - testx.^2 + testx + 150*randn(size(testx));

%% Run a cross-validation over model orders
maxorder = 7;
X = [];
testX = [];
K = 10 %K-fold CV
sizes = repmat(floor(N/K),1,K);
sizes(end) = sizes(end) + N - sum(sizes);
csizes = [0 cumsum(sizes)];

% Note that it is often sensible to permute the data objects before
% performing CV.  It is not necessary here as x was created randomly.  If
% it were necessary, the following code would work:
% order = randperm(N);
% x = x(order); Or: X = X(order,:) if it is multi-dimensional.
% t = t(order);

for k = 0:maxorder
    X = [X x.^k];
    testX = [testX testx.^k];
    for fold = 1:K
        % Partition the data
        % foldX contains the data for just one fold
        % trainX contains all other data
        
        foldX = X(csizes(fold)+1:csizes(fold+1),:);
        trainX = X;
        trainX(csizes(fold)+1:csizes(fold+1),:) = [];
        foldt = t(csizes(fold)+1:csizes(fold+1));
        traint = t;
        traint(csizes(fold)+1:csizes(fold+1)) = [];
        
        w = inv(trainX'*trainX)*trainX'*traint;
        fold_pred = foldX*w;
        cv_loss(fold,k+1) = mean((fold_pred-foldt).^2);
        ind_pred = testX*w;
        ind_loss(fold,k+1) = mean((ind_pred - testt).^2);
        train_pred = trainX*w;
        train_loss(fold,k+1) = mean((train_pred - traint).^2);
    end
end

%% Plot the results
figure(1);
subplot(131)
plot(0:maxorder,mean(cv_loss,1),'linewidth',2)
xlabel('Model Order');
ylabel('Loss');
title('CV Loss');
subplot(132)
plot(0:maxorder,mean(train_loss,1),'linewidth',2)
xlabel('Model Order');
ylabel('Loss');
title('Train Loss');
subplot(133)
plot(0:maxorder,mean(ind_loss,1),'linewidth',2)
xlabel('Model Order');
ylabel('Loss');
title('Independent Test Loss')
##### SOURCE END #####
--></body></html>